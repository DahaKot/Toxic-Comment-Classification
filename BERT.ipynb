{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_train = pd.read_csv(\"./Data/jigsaw-toxic-comment-train.csv\")\n",
    "comments_test  = pd.read_csv(\"./Data/validation.csv\")\n",
    "bias_train = pd.read_csv(\"./Data/jigsaw-unintended-bias-train.csv\")\n",
    "\n",
    "# drop unneeded columns right away\n",
    "bias_train = bias_train.iloc[:, :3]\n",
    "comments_train = comments_train.iloc[:, :3]\n",
    "# print (\"Train : \", len(comments_train), \"Test : \", len(comments_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_toxic_comment['comment_text'] = [train_data_toxic_comment['comment_text'].replace('\\\"', '')]\n",
    "# train_data_toxic_comment['comment_text'] = train_data_toxic_comment['comment_text'].replace('\\n', '')\n",
    "# train_data_toxic_comment['comment_text'] = train_data_toxic_comment['comment_text'].lower()\n",
    "\n",
    "\n",
    "# comments_train['comment_text'] = [sent.replace('\\\"', '') for sent in comments_train['comment_text']]\n",
    "# comments_train['comment_text'] = [sent.replace('\\n', '') for sent in comments_train['comment_text']]\n",
    "# comments_train['comment_text'] = [sent.lower()           for sent in comments_train['comment_text']]\n",
    "# comments_train['comment_text'] = [sent[:512]             for sent in comments_train['comment_text']]\n",
    "\n",
    "# comments_test['comment_text'] = [sent.replace('\\\"', '') for sent in comments_test['comment_text']]\n",
    "# comments_test['comment_text'] = [sent.replace('\\n', '') for sent in comments_test['comment_text']]\n",
    "# comments_test['comment_text'] = [sent.lower()           for sent in comments_test['comment_text']]\n",
    "# comments_test['comment_text'] = [sent[:512]             for sent in comments_test['comment_text']]\n",
    "\n",
    "# make_labels is similar to round but now we control what 0.5 is equal to\n",
    "def make_labels(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i] = 0 if x[i] <= 0.5 else 1\n",
    "    return x\n",
    "\n",
    "# bias_train[\"toxic\"] = make_labels(bias_train[\"toxic\"])\n",
    "bias_train[\"toxic\"] = bias_train[\"toxic\"].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity fraction in train:  0.06285331763999694\n",
      "toxicity fraction in test:  0.15375\n"
     ]
    }
   ],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.replace('\\\"', '')\n",
    "    sentence = sentence.replace('\\n', '')\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence[:256]\n",
    "    return sentence\n",
    "\n",
    "comments_train = pd.concat([comments_train, bias_train])\n",
    "\n",
    "comments_train['comment_text'] = comments_train['comment_text'].apply(preprocess)\n",
    "comments_test['comment_text']  = comments_test['comment_text'].apply(preprocess)\n",
    "print(\"toxicity fraction in train: \", comments_train['toxic'].value_counts()[1] / comments_train['toxic'].count())\n",
    "print(\"toxicity fraction in test: \", comments_test['toxic'].value_counts()[1] / comments_test['toxic'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity fraction in train:  0.18212674309237878\n",
      "toxicity fraction in test:  0.15375\n"
     ]
    }
   ],
   "source": [
    "# this cell is dealing with class imbalance\n",
    "# if you do not agree on the method - change it\n",
    "toxic = comments_train[comments_train[\"toxic\"] == 1]\n",
    "untoxic = comments_train[comments_train[\"toxic\"] == 0].sample(600000)\n",
    "\n",
    "comments_train = pd.concat((toxic, untoxic))\n",
    "# extra_toxic_comments = comments_train[comments_train[\"toxic\"] == 1].sample(10000)\n",
    "# comments_train = pd.concat([comments_train, extra_toxic_comments])\n",
    "\n",
    "print(\"toxicity fraction in train: \", comments_train['toxic'].value_counts()[1] / comments_train['toxic'].count())\n",
    "print(\"toxicity fraction in test: \", comments_test['toxic'].value_counts()[1] / comments_test['toxic'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences train :  733610 , test :  8000\n",
      "Example :  [CLS] mesaj sayfama yaptığınız müdahale vandalizm olarak kabul edilir. lütfen reklam yapmak için kullanıcıların sayfalarını değiştirmeyinizdelamorena  [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentences_train = np.array(comments_train['comment_text'].values)\n",
    "sentences_test  = np.array(comments_test ['comment_text'].values)\n",
    "sentences_train = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences_train]\n",
    "sentences_test  = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences_test]\n",
    "\n",
    "labels_train = np.array(comments_train['toxic'])\n",
    "labels_test  = np.array(comments_test['toxic'])\n",
    "\n",
    "assert len(sentences_train) == len(labels_train)\n",
    "assert len(sentences_test)  == len(labels_test)\n",
    "\n",
    "print (\"Number of sentences train : \", len(sentences_train), \", test : \", len(sentences_test))\n",
    "print (\"Example : \", sentences_test[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733610/733610 [08:56<00:00, 1367.66it/s]\n",
      "100%|██████████| 8000/8000 [00:07<00:00, 1009.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'co', '##cks', '##uck', '##er', 'before', 'you', 'pis', '##s', 'around', 'on', 'my', 'work', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import BertTokenizer, BertConfig\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "tokenized_texts_tr = [tokenizer.tokenize(sent) for sent in tqdm(sentences_train)]\n",
    "tokenized_texts_te = [tokenizer.tokenize(sent) for sent in tqdm(sentences_test)]\n",
    "print (tokenized_texts_tr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 5505\n",
    "# print (\"Language : [\", comments_test['lang'][i], \"]\")\n",
    "# print (sentences_test[i])\n",
    "# print(tokenized_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_and_masks(tokenizer, tokenized_texts):\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenized_texts)]\n",
    "    input_ids = pad_sequences(\n",
    "        input_ids,\n",
    "        maxlen=256,\n",
    "        dtype=\"long\",\n",
    "        truncating=\"post\",\n",
    "        padding=\"post\"\n",
    "    )\n",
    "    attention_masks = [[float(i>0) for i in seq] for seq in tqdm(input_ids)]\n",
    "    print (\"Dataset shape : \", input_ids.shape)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 733610/733610 [00:40<00:00, 17939.13it/s]\n",
      "100%|██████████| 733610/733610 [02:15<00:00, 5394.47it/s]\n",
      " 16%|█▌        | 1291/8000 [00:00<00:00, 12904.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape :  (733610, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:00<00:00, 12649.41it/s]\n",
      "100%|██████████| 8000/8000 [00:01<00:00, 5294.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape :  (8000, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids_tr, attention_masks_tr = ids_and_masks(tokenizer, tokenized_texts_tr)\n",
    "input_ids_te, attention_masks_te = ids_and_masks(tokenizer, tokenized_texts_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_tr = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenized_texts_tr)]\n",
    "# input_ids_tr = pad_sequences(\n",
    "#     input_ids_tr,\n",
    "#     maxlen=512,\n",
    "#     dtype=\"long\",\n",
    "#     truncating=\"post\",\n",
    "#     padding=\"post\"\n",
    "# )\n",
    "# print (input_ids_tr.shape)\n",
    "# attention_masks_tr = [[float(i>0) for i in seq] for seq in input_ids_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_te = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(tokenized_texts_te)]\n",
    "# input_ids_te = pad_sequences(\n",
    "#     input_ids_te,\n",
    "#     maxlen=512,\n",
    "#     dtype=\"long\",\n",
    "#     truncating=\"post\",\n",
    "#     padding=\"post\"\n",
    "# )\n",
    "# print (input_ids_te.shape)\n",
    "# attention_masks_te = [[float(i>0) for i in seq] for seq in input_ids_te]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(input_ids_tr)\n",
    "train_labels = torch.tensor(labels_train, dtype=torch.long)\n",
    "train_masks  = torch.tensor(attention_masks_tr)\n",
    "\n",
    "test_inputs  = torch.tensor(input_ids_te)\n",
    "test_labels  = torch.tensor(labels_test, dtype=torch.long)\n",
    "test_masks   = torch.tensor(attention_masks_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    sampler=RandomSampler(train_data),\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    sampler=SequentialSampler(test_data),\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import AdamW, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss на обучающей выборке: 0.23270\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Будем сохранять loss во время обучения\n",
    "# и рисовать график в режиме реального времени\n",
    "train_loss_set = []\n",
    "train_loss = 0\n",
    "\n",
    "\n",
    "# Обучение\n",
    "# Переводим модель в training mode\n",
    "model.train()\n",
    "\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    # добавляем батч для вычисления на GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Распаковываем данные из dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # если не сделать .zero_grad(), градиенты будут накапливаться\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    train_loss_set.append(loss[0].item())  \n",
    "    \n",
    "    # Backward pass\n",
    "    loss[0].backward()\n",
    "    \n",
    "    # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
    "    optimizer.step()\n",
    "\n",
    "    # Обновляем loss\n",
    "    train_loss += loss[0].item()\n",
    "    \n",
    "    # Рисуем график\n",
    "    clear_output(True)\n",
    "    plt.plot(train_loss_set)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:34<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных предсказаний на валидационной выборке: 84.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Валидация\n",
    "# Переводим модель в evaluation mode\n",
    "model.eval()\n",
    "\n",
    "valid_preds, valid_labels = [], []\n",
    "\n",
    "for batch in tqdm(test_dataloader):   \n",
    "    # добавляем батч для вычисления на GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Распаковываем данные из dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
    "    # Это ускорит процесс предсказания меток для валидационных данных.\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
    "    logits = logits[0].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "#     print (label_ids.shape)\n",
    "    batch_preds = np.argmax(logits, axis=1)\n",
    "#     batch_labels = np.concatenate(label_ids)\n",
    "    batch_labels = np.array(label_ids)\n",
    "\n",
    "    valid_preds.extend(batch_preds)\n",
    "    valid_labels.extend(batch_labels)\n",
    "\n",
    "print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n",
    "    accuracy_score(valid_labels, valid_preds) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.813875 0.032375 0.12625  0.0275  ] [0.15375] [0.84625]\n",
      "Custom model\n",
      "17.88617886178862 % of toxic comments were predicted correctly\n",
      "82.11382113821138 % of toxic comments were marked non-toxic\n",
      "96.17429837518465 % of non-toxic comments were predicted correctly\n",
      "3.825701624815362 % of non-toxic comments were marked toxic\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(valid_labels, valid_preds).ravel()\n",
    "\n",
    "valid_labels = np.array(valid_labels)\n",
    "print(np.array((tn, fp, fn, tp)) / valid_labels.shape[0],\n",
    "      valid_labels.sum() / valid_labels.shape,\n",
    "      (valid_labels.shape[0] - valid_labels.sum()) / valid_labels.shape)\n",
    "\n",
    "n_toxic = valid_labels.sum()\n",
    "n_non_toxic = valid_labels.shape[0] - valid_labels.sum()\n",
    "\n",
    "print(\"Custom model\")\n",
    "print((tp / n_toxic).item() * 100, \"% of toxic comments were predicted correctly\")\n",
    "print((fn / n_toxic).item() * 100, \"% of toxic comments were marked non-toxic\")\n",
    "print((tn / n_non_toxic).item() * 100, \"% of non-toxic comments were predicted correctly\")\n",
    "print((fp / n_non_toxic).item() * 100, \"% of non-toxic comments were marked toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703023861848664"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.roc_auc_score(valid_labels, valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in sentences:\n",
    "#     sentence = sentence.replace('\\\"', '')\n",
    "#     sentence = sentence.replace('\\n', '')\n",
    "#     sentence = sentence.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = '\"Some sentence\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.replace(\"\\\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_len(x):\n",
    "#     return len(x.split())\n",
    "\n",
    "# train_data_toxic_comment[\"len\"] = train_data_toxic_comment[\"comment_text\"].apply(make_len)\n",
    "# train_data_toxic_comment[\"len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_toxic_comment.count() - no missing values\n",
    "# train_data_toxic_comment.nunique() - no repeated comments\n",
    "# train_data_toxic_comment['toxic'].value_counts() - imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_unintended_bias = pd.read_csv(\"./Data/jigsaw-unintended-bias-train.csv\")\n",
    "# # train_data_unintended_bias.count() - there are missing values in columns which describe person from the comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of NaNs here, so I will replace them with something interesting. Some features are from 0 and 1, another are strictly categorical. We still have much more good comments than toxic ones - so again imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert \"approved\" to 1 and \"rejected\" to 0\n",
    "# train_data_unintended_bias[\"rating\"] = train_data_unintended_bias[\"rating\"].apply(lambda x: int(x == \"approved\"))\n",
    "\n",
    "# # 8:32 - there are columns which describe person from the comment\n",
    "# categorical_mean = train_data_unintended_bias.iloc[:, 8:32].mean()\n",
    "# train_data_unintended_bias.iloc[:, 8:32] = train_data_unintended_bias.iloc[:, 8:32].fillna(categorical_mean)\n",
    "\n",
    "# train_data_unintended_bias[\"len\"] = train_data_unintended_bias[\"comment_text\"].apply(make_len)\n",
    "# train_data_unintended_bias[\"len\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_unintended_bias.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
