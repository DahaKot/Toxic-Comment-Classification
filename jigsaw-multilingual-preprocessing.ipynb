{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "This notebook preprocesses our dataset for compatibility with BERT. You should feel free to investigate other solutions (both models and tokenizers)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\r\n",
      "  Downloading bert_tensorflow-1.0.1-py2.py3-none-any.whl (67 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 67 kB 1.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from bert-tensorflow) (1.14.0)\r\n",
      "Installing collected packages: bert-tensorflow\r\n",
      "Successfully installed bert-tensorflow-1.0.1\r\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# We'll use a tokenizer for the BERT model from the modelling demo notebook.\n",
    "!pip install bert-tensorflow\n",
    "import bert.tokenization\n",
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set global variables\n",
    "\n",
    "Set maximum sequence length and path variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "DATA_PATH =  \"../input/jigsaw-multilingual-toxic-comment-classification\"\n",
    "BERT_PATH = \"../input/bert-multi\"\n",
    "BERT_PATH_SAVEDMODEL = os.path.join(BERT_PATH, \"bert_multi_from_tfhub\")\n",
    "\n",
    "OUTPUT_PATH = \"/kaggle/working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "\n",
    "Load and look at examples from [our first competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/). These are comments from Wikipedia with a variety of annotations (toxic, obscene, threat, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data from our first competition,\n",
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "wiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\"\n",
    "\n",
    "wiki_toxic_comment_train = pandas.read_csv(os.path.join(\n",
    "    DATA_PATH, wiki_toxic_comment_data))\n",
    "wiki_toxic_comment_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenizer\n",
    "\n",
    "Get the tokenizer corresponding to our multilingual BERT model. See [TensorFlow \n",
    "Hub](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1) for more information about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(bert_path=BERT_PATH_SAVEDMODEL):\n",
    "    \"\"\"Get the tokenizer for a BERT layer.\"\"\"\n",
    "    bert_layer = tf.saved_model.load(bert_path)\n",
    "    bert_layer = hub.KerasLayer(bert_layer, trainable=False)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    cased = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tf.gfile = tf.io.gfile  # for bert.tokenization.load_vocab in tokenizer\n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file, cased)\n",
    "  \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at one of our example sentences after we tokenize it, and then again after converting it to word IDs for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretty much everyone from warren county/surrounding regions was born at glens falls hospital. myself included. however, i'm not sure this qualifies an\n",
      "['pretty', 'much', 'everyone', 'from', 'war', '##ren', 'county', '/', 'surrounding', 'regions', 'was', 'born', 'at', 'g', '##lens', 'falls', 'hospital']\n",
      "[108361, 13172, 48628, 10188, 10338, 10969, 17382, 120, 27027, 21721, 10134, 11175, 10160, 175, 97681, 35017, 18141]\n"
     ]
    }
   ],
   "source": [
    "example_sentence = wiki_toxic_comment_train.iloc[37].comment_text[:150]\n",
    "print(example_sentence)\n",
    "\n",
    "example_tokens = tokenizer.tokenize(example_sentence)\n",
    "print(example_tokens[:17])\n",
    "\n",
    "example_input_ids = tokenizer.convert_tokens_to_ids(example_tokens)\n",
    "print(example_input_ids[:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Process individual sentences for input to BERT using the tokenizer, and then prepare the entire dataset. The same code will process the other training data files, as well as the validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 examples in 29.022007942199707\n",
      "Processed 20000 examples in 57.7885365486145\n",
      "Processed 30000 examples in 85.57034993171692\n",
      "Processed 40000 examples in 115.31633424758911\n",
      "Processed 50000 examples in 143.29948663711548\n",
      "Processed 60000 examples in 172.64583325386047\n",
      "Processed 70000 examples in 201.35567164421082\n",
      "Processed 80000 examples in 230.33242917060852\n",
      "Processed 90000 examples in 258.0896406173706\n",
      "Processed 100000 examples in 286.4155900478363\n",
      "Processed 110000 examples in 315.519428730011\n",
      "Processed 120000 examples in 343.060733795166\n",
      "Processed 130000 examples in 371.3235728740692\n",
      "Processed 140000 examples in 399.7340397834778\n",
      "Processed 150000 examples in 428.1987564563751\n",
      "Processed 160000 examples in 456.4187214374542\n",
      "Processed 170000 examples in 484.2759895324707\n",
      "Processed 180000 examples in 512.1130759716034\n",
      "Processed 190000 examples in 539.8287353515625\n",
      "Processed 200000 examples in 567.6800787448883\n",
      "Processed 210000 examples in 595.8626999855042\n",
      "Processed 220000 examples in 623.9759175777435\n",
      "Processed 230000 examples in 634.5377519130707\n"
     ]
    }
   ],
   "source": [
    "def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n",
    "    \"\"\"Helper function to prepare data for BERT. Converts sentence input examples\n",
    "    into the form ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n",
    "    # Tokenize, and truncate to max_seq_length if necessary.\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    if len(tokens) > max_seq_length - 2:\n",
    "        tokens = tokens[:(max_seq_length - 2)]\n",
    "\n",
    "    # Convert the tokens in the sentence to word IDs.\n",
    "    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    pad_length = max_seq_length - len(input_ids)\n",
    "    input_ids.extend([0] * pad_length)\n",
    "    input_mask.extend([0] * pad_length)\n",
    "\n",
    "    # We only have one input segment.\n",
    "    segment_ids = [0] * max_seq_length\n",
    "\n",
    "    return (input_ids, input_mask, segment_ids)\n",
    "\n",
    "def preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n",
    "                                seq_length=SEQUENCE_LENGTH, verbose=True):\n",
    "    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n",
    "    and save the result.\"\"\"\n",
    "    dataframe = pandas.read_csv(os.path.join(DATA_PATH, unprocessed_filename),\n",
    "                                index_col='id')\n",
    "    processed_filename = (unprocessed_filename.rstrip('.csv') +\n",
    "                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n",
    "\n",
    "    pos = 0\n",
    "    start = time.time()\n",
    "\n",
    "    while pos < len(dataframe):\n",
    "        processed_df = dataframe[pos:pos + 10000].copy()\n",
    "\n",
    "        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['all_segment_id'] = (\n",
    "            zip(*processed_df[text_label].apply(process_sentence)))\n",
    "\n",
    "        if pos == 0:\n",
    "            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n",
    "        else:\n",
    "            processed_df.to_csv(processed_filename, index_label='id', mode='a',\n",
    "                                header=False)\n",
    "\n",
    "        if verbose:\n",
    "            print('Processed {} examples in {}'.format(\n",
    "                pos + 10000, time.time() - start))\n",
    "        pos += 10000\n",
    "    return\n",
    "  \n",
    "# Process the training dataset.\n",
    "preprocess_and_save_dataset(wiki_toxic_comment_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
